{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# korea univ morpheme analyzer\n",
    "sentence = '이를 위해서도 독도가 일본 영토라는 억지 주장이나 군국주의 망령에 사로잡힌 야스쿠니 신사참배 같은 근시안적인 선거공약을 마련해서는 안될 것이다.'\n",
    "sentence = '실제로 대우가 현재 분양중인 일산 대우1차아파트의 37평형에 관한 팩스정보를 얻으려면 일반 전화기로 02-759-3841∼2를 돌린 뒤 \"01(분양안내), 02(분양금납부안내), 03(분양금입금안내), 04(분양정보팩스전송서비스)중 하나를 선택하라\"는 음성이 흘러 나오면 04를 선택한다.'\n",
    "sentence = '4월7~8일 : 미국영화협회(MPAA) 제프리 하디 아시아태평양 담당 부회장이 문화관광부와 산업자원부를 방문해 \"스크린쿼터 완화할 경우 5억달러 정도를 투자해 전국 20개 지역에 스크린 10개짜리 멀티플렉스를 짓겠다\"고 제의.'\n",
    "\n",
    "m_command = 'cd data/kmat/bin/;./kmat <<<\\\"' + sentence + '\\\" 2>/dev/null'\n",
    "result = subprocess.check_output(m_command.encode(encoding='cp949', errors='ignore'), shell=True,\n",
    "                                 executable='/bin/bash', )\n",
    "\n",
    "mor_name_lists = []\n",
    "mor_tags_lists = []\n",
    "\n",
    "for each in result.decode(encoding='cp949', errors='ignore').split('\\n'):\n",
    "    if len(each) > 0:\n",
    "        try:\n",
    "            mor_texts = each.split('\\t')[1]\n",
    "        except:\n",
    "            print(each)\n",
    "        mor_results = mor_texts.split('+')\n",
    "        \n",
    "        for each_mor in mor_results:\n",
    "            mor_name_lists.append(each_mor.split('/')[0])\n",
    "            mor_tags_lists.append(each_mor.split('/')[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from data_utils import get_trimmed_glove_vectors, load_vocab, \\\n",
    "    get_processing_word, Data\n",
    "from config import Config\n",
    "from cnn_lstm_crf import CnnLstmCrfModel\n",
    "import subprocess\n",
    "\n",
    "def get_mor_result(sentence):\n",
    "    # korea univ morpheme analyzer\n",
    "    m_command = \"cd data/kmat/bin/;./kmat <<<\\'\" + sentence + \"\\' 2>/dev/null\"\n",
    "    result = subprocess.check_output(m_command.encode(encoding='cp949', errors='ignore'), shell=True,\n",
    "                                     executable='/bin/bash')\n",
    "    mor_name_lists = []\n",
    "    mor_tags_lists = []\n",
    "\n",
    "    for each in result.decode(encoding='cp949', errors='ignore').split('\\n'):\n",
    "        if len(each) > 0:\n",
    "            try:\n",
    "                mor_texts = each.split('\\t')[1]\n",
    "            except:\n",
    "                print(each)\n",
    "            mor_results = mor_texts.split('+')\n",
    "\n",
    "            for each_mor in mor_results:\n",
    "                mor_name_lists.append(each_mor.split('/')[0])\n",
    "                mor_tags_lists.append(each_mor.split('/')[1])\n",
    "\n",
    "    return mor_name_lists, mor_tags_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gradients_impl.py:95: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "from data_utils import get_trimmed_glove_vectors, load_vocab, \\\n",
    "    get_processing_word, Data\n",
    "from config import Config\n",
    "from cnn_lstm_crf import CnnLstmCrfModel\n",
    "\n",
    "config = Config()\n",
    "\n",
    "vocab_words = load_vocab(config.words_filename)\n",
    "vocab_mor_tags = load_vocab(config.mor_tags_filename)\n",
    "vocab_tags  = load_vocab(config.tags_filename)\n",
    "vocab_chars = load_vocab(config.chars_filename)\n",
    "vocab_lex_tags = load_vocab(config.lex_tags_filename)\n",
    "\n",
    "# get processing functions\n",
    "processing_word = get_processing_word(vocab_words, vocab_chars,\n",
    "                lowercase=True, chars=config.chars)\n",
    "processing_mor_tag = get_processing_word(vocab_mor_tags, lowercase=False)\n",
    "processing_tag = get_processing_word(vocab_tags,\n",
    "                lowercase=False)\n",
    "processing_lex_tag = get_processing_word(vocab_lex_tags, lowercase=False)\n",
    "\n",
    "\n",
    "# get pre trained embeddings\n",
    "embeddings = get_trimmed_glove_vectors(config.trimmed_filename)\n",
    "\n",
    "# create dataset\n",
    "dev   = Data(config.dev_filename, processing_word, processing_mor_tag,\n",
    "             processing_lex_tag, processing_tag, config.max_iter)\n",
    "test  = Data(config.test_filename, processing_word, processing_mor_tag,\n",
    "             processing_lex_tag, processing_tag, config.max_iter)\n",
    "train = Data(config.train_filename, processing_word, processing_mor_tag,\n",
    "             processing_lex_tag, processing_tag, config.max_iter)\n",
    "\n",
    "\n",
    "cnn_model = CnnLstmCrfModel(config, embeddings, ntags=len(vocab_tags),nchars=len(vocab_chars))\n",
    "cnn_model.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from results/full_cnn_with_mor_lex_new_data_0924/model_fil_2345_600_100_128_drop_0.8_0924.weights\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring parameters from results/full_cnn_with_mor_lex_new_data_0924/model_fil_2345_600_100_128_drop_0.8_0924.weights\n"
     ]
    }
   ],
   "source": [
    "cnn_model.write_tag_result(vocab_tags, processing_word, processing_mor_tag, processing_lex_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cnn_model.interactive_shell(vocab_tags, processing_word, processing_mor_tag, processing_lex_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_text = '이를 위해서도 독도가 일본 영토라는 억지 주장이나 군국주의 망령에 사로잡힌 야스쿠니 신사참배 같은 근시안적인 선거공약을 마련해서는 안될 것이다.'\n",
    "m_command = 'cd data/kmat/bin/;./kmat <<<\\\"' + input_text + '\\\" 2>/dev/null'\n",
    "result = subprocess.check_output(m_command.encode(encoding='cp949',errors='ignore'), shell=True, executable='/bin/bash',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이를\t이/NP+를/JKO\n",
      "위해서도\t위하/VV+아서/EM+도/JX\n",
      "독도가\t독도/NNP+가/JKS\n",
      "일본\t일본/NNP\n",
      "영토라는\t영토/NNG+이/VCP+라는/ETM\n",
      "억지\t억지/NNG\n",
      "주장이나\t주장/NNG+이나/JKB\n",
      "군국주의\t군국주의/NNG\n",
      "망령에\t망령/NNG+에/JKB\n",
      "사로잡힌\t사로잡히/VV+ㄴ/ETM\n",
      "야스쿠니\t야스쿠니/NNP\n",
      "신사참배\t신사/NNG+참배/NNG\n",
      "같은\t같/VA+은/ETM\n",
      "근시안적인\t근시안적/NNG+이/VCP+ㄴ/ETM\n",
      "선거공약을\t선거/NNG+공약/NNG+을/JKO\n",
      "마련해서는\t마련/NNG+하/XSV+아서/EM+는/JX\n",
      "안될\t안/MAG+되/XSV+ㄹ/ETM\n",
      "것이다.\t것/NNB+이/VCP+다/EM+./SF\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(result.decode(encoding='cp949',errors='ignore'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mor_lists = []\n",
    "for each in result.decode(encoding='cp949',errors='ignore').split('\\n'):\n",
    "    if len(each) > 0:\n",
    "        try:\n",
    "            mor_text = each.split('\\t')[1]\n",
    "        except:\n",
    "            print(each)\n",
    "        mor_results = mor_text.split('+')\n",
    "\n",
    "        for each_mor in mor_results:\n",
    "            mor_lists.append(each_mor.split('/')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['이',\n",
       " '를',\n",
       " '위하',\n",
       " '아서',\n",
       " '도',\n",
       " '독도',\n",
       " '가',\n",
       " '일본',\n",
       " '영토',\n",
       " '이',\n",
       " '라는',\n",
       " '억지',\n",
       " '주장',\n",
       " '이나',\n",
       " '군국주의',\n",
       " '망령',\n",
       " '에',\n",
       " '사로잡히',\n",
       " 'ㄴ',\n",
       " '야스쿠니',\n",
       " '신사',\n",
       " '참배',\n",
       " '같',\n",
       " '은',\n",
       " '근시안적',\n",
       " '이',\n",
       " 'ㄴ',\n",
       " '선거',\n",
       " '공약',\n",
       " '을',\n",
       " '마련',\n",
       " '하',\n",
       " '아서',\n",
       " '는',\n",
       " '안',\n",
       " '되',\n",
       " 'ㄹ',\n",
       " '것',\n",
       " '이',\n",
       " '다',\n",
       " '.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mor_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
